{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks- ResNet\n",
    "\n",
    "\n",
    "![alt text](r.png \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding additional / new layers would not hurt the model’s performance as regularisation will skip over them if those layers were not useful.\n",
    "\n",
    "\n",
    "- If the additional / new layers were useful, even with the presence of regularisation, the weights or kernels of the layers will be non-zero and model performance could increase slightly.\n",
    "\n",
    "\n",
    "- Therefore, by adding new layers, because of the “Skip connection” / “residual connection”, it is guaranteed that performance of the model does not decrease but it could increase slightly.\n",
    "\n",
    "- By stacking these ResNet blocks on top of each other, you can form a very deep network. Having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main types of blocks are used in a ResNet:\n",
    "\n",
    "\n",
    "1.The identity block — same as the one we saw above. The identity block is the standard block used in ResNets and corresponds to the case where the input activation has the same dimension as the output activation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.The Convolutional block — We can use this type of block when the input and output dimensions don’t match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](rr1.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnetimage](https://user-images.githubusercontent.com/30661597/78585170-f4ac7c80-786b-11ea-8b00-8b751c65f5ca.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 3 layers.see colors in dia. each has different no of channels . each layer has 2 residual blocks\n",
    "# function to make layers : make_layer . takes block,channels, layers(no of residual block in each layer), stride as arg\n",
    "# layers will be a list [2,2,2] since each layer has 2 residual blocks\n",
    "# layer[0] - first layer\n",
    "# condition for downsampling:\n",
    "# 1) when i/p size not equal to o/p size\n",
    "# 2) when the size of feature map of i /p not equal to size of feature map of o /p - this can be found using stride\n",
    "# when stride is not 1, then say 2, then the i/p size has been downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])  \n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8) # size of feature map is 8\n",
    "        self.fc = nn.Linear(64, num_classes) # 64 channels at final layer that willbe i/p to final layer\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                                       nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels # i/p channel changes for next layer.prev layer o/p becomes i/p to next\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels)) # append 2nd residual block\n",
    "        return nn.Sequential(*layers)  # *before list of layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create object of ResNet class and loss and activation function for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [100/500] Loss: 1.6557\n",
      "Epoch [1/25], Step [200/500] Loss: 1.2898\n",
      "Epoch [1/25], Step [300/500] Loss: 1.3153\n",
      "Epoch [1/25], Step [400/500] Loss: 1.2694\n",
      "Epoch [1/25], Step [500/500] Loss: 1.1604\n",
      "Epoch [2/25], Step [100/500] Loss: 1.1571\n",
      "Epoch [2/25], Step [200/500] Loss: 1.1095\n",
      "Epoch [2/25], Step [300/500] Loss: 0.9807\n",
      "Epoch [2/25], Step [400/500] Loss: 1.0693\n",
      "Epoch [2/25], Step [500/500] Loss: 0.8838\n",
      "Epoch [3/25], Step [100/500] Loss: 0.9405\n",
      "Epoch [3/25], Step [200/500] Loss: 0.9049\n",
      "Epoch [3/25], Step [300/500] Loss: 0.8155\n",
      "Epoch [3/25], Step [400/500] Loss: 0.8773\n",
      "Epoch [3/25], Step [500/500] Loss: 0.6850\n",
      "Epoch [4/25], Step [100/500] Loss: 0.7337\n",
      "Epoch [4/25], Step [200/500] Loss: 0.6807\n",
      "Epoch [4/25], Step [300/500] Loss: 0.9716\n",
      "Epoch [4/25], Step [400/500] Loss: 0.9532\n",
      "Epoch [4/25], Step [500/500] Loss: 0.8800\n",
      "Epoch [5/25], Step [100/500] Loss: 0.6968\n",
      "Epoch [5/25], Step [200/500] Loss: 0.7616\n",
      "Epoch [5/25], Step [300/500] Loss: 0.7219\n",
      "Epoch [5/25], Step [400/500] Loss: 0.6923\n",
      "Epoch [5/25], Step [500/500] Loss: 0.6200\n",
      "Epoch [6/25], Step [100/500] Loss: 0.6126\n",
      "Epoch [6/25], Step [200/500] Loss: 0.6976\n",
      "Epoch [6/25], Step [300/500] Loss: 0.5624\n",
      "Epoch [6/25], Step [400/500] Loss: 0.5853\n",
      "Epoch [6/25], Step [500/500] Loss: 0.7961\n",
      "Epoch [7/25], Step [100/500] Loss: 0.5909\n",
      "Epoch [7/25], Step [200/500] Loss: 0.7478\n",
      "Epoch [7/25], Step [300/500] Loss: 0.6783\n",
      "Epoch [7/25], Step [400/500] Loss: 0.6210\n",
      "Epoch [7/25], Step [500/500] Loss: 0.6833\n",
      "Epoch [8/25], Step [100/500] Loss: 0.4743\n",
      "Epoch [8/25], Step [200/500] Loss: 0.4535\n",
      "Epoch [8/25], Step [300/500] Loss: 0.3930\n",
      "Epoch [8/25], Step [400/500] Loss: 0.7335\n",
      "Epoch [8/25], Step [500/500] Loss: 0.5062\n",
      "Epoch [9/25], Step [100/500] Loss: 0.4935\n",
      "Epoch [9/25], Step [200/500] Loss: 0.5007\n",
      "Epoch [9/25], Step [300/500] Loss: 0.5656\n",
      "Epoch [9/25], Step [400/500] Loss: 0.3940\n",
      "Epoch [9/25], Step [500/500] Loss: 0.5017\n",
      "Epoch [10/25], Step [100/500] Loss: 0.5317\n",
      "Epoch [10/25], Step [200/500] Loss: 0.6691\n",
      "Epoch [10/25], Step [300/500] Loss: 0.5417\n",
      "Epoch [10/25], Step [400/500] Loss: 0.6996\n",
      "Epoch [10/25], Step [500/500] Loss: 0.6654\n",
      "Epoch [11/25], Step [100/500] Loss: 0.5873\n",
      "Epoch [11/25], Step [200/500] Loss: 0.5209\n",
      "Epoch [11/25], Step [300/500] Loss: 0.5781\n",
      "Epoch [11/25], Step [400/500] Loss: 0.4315\n",
      "Epoch [11/25], Step [500/500] Loss: 0.5939\n",
      "Epoch [12/25], Step [100/500] Loss: 0.4845\n",
      "Epoch [12/25], Step [200/500] Loss: 0.6054\n",
      "Epoch [12/25], Step [300/500] Loss: 0.3845\n",
      "Epoch [12/25], Step [400/500] Loss: 0.5146\n",
      "Epoch [12/25], Step [500/500] Loss: 0.5893\n",
      "Epoch [13/25], Step [100/500] Loss: 0.4201\n",
      "Epoch [13/25], Step [200/500] Loss: 0.4751\n",
      "Epoch [13/25], Step [300/500] Loss: 0.4638\n",
      "Epoch [13/25], Step [400/500] Loss: 0.4640\n",
      "Epoch [13/25], Step [500/500] Loss: 0.5020\n",
      "Epoch [14/25], Step [100/500] Loss: 0.5132\n",
      "Epoch [14/25], Step [200/500] Loss: 0.5913\n",
      "Epoch [14/25], Step [300/500] Loss: 0.4598\n",
      "Epoch [14/25], Step [400/500] Loss: 0.4983\n",
      "Epoch [14/25], Step [500/500] Loss: 0.4634\n",
      "Epoch [15/25], Step [100/500] Loss: 0.4113\n",
      "Epoch [15/25], Step [200/500] Loss: 0.4223\n",
      "Epoch [15/25], Step [300/500] Loss: 0.3569\n",
      "Epoch [15/25], Step [400/500] Loss: 0.4286\n",
      "Epoch [15/25], Step [500/500] Loss: 0.3916\n",
      "Epoch [16/25], Step [100/500] Loss: 0.4040\n",
      "Epoch [16/25], Step [200/500] Loss: 0.3883\n",
      "Epoch [16/25], Step [300/500] Loss: 0.5932\n",
      "Epoch [16/25], Step [400/500] Loss: 0.3443\n",
      "Epoch [16/25], Step [500/500] Loss: 0.3045\n",
      "Epoch [17/25], Step [100/500] Loss: 0.5030\n",
      "Epoch [17/25], Step [200/500] Loss: 0.3654\n",
      "Epoch [17/25], Step [300/500] Loss: 0.4280\n",
      "Epoch [17/25], Step [400/500] Loss: 0.4130\n",
      "Epoch [17/25], Step [500/500] Loss: 0.3432\n",
      "Epoch [18/25], Step [100/500] Loss: 0.3396\n",
      "Epoch [18/25], Step [200/500] Loss: 0.4338\n",
      "Epoch [18/25], Step [300/500] Loss: 0.3547\n",
      "Epoch [18/25], Step [400/500] Loss: 0.4255\n",
      "Epoch [18/25], Step [500/500] Loss: 0.4088\n",
      "Epoch [19/25], Step [100/500] Loss: 0.4348\n",
      "Epoch [19/25], Step [200/500] Loss: 0.4345\n",
      "Epoch [19/25], Step [300/500] Loss: 0.2949\n",
      "Epoch [19/25], Step [400/500] Loss: 0.3266\n",
      "Epoch [19/25], Step [500/500] Loss: 0.3218\n",
      "The new learning rate is 0.0005\n",
      "Epoch [20/25], Step [100/500] Loss: 0.3600\n",
      "Epoch [20/25], Step [200/500] Loss: 0.3100\n",
      "Epoch [20/25], Step [300/500] Loss: 0.3065\n",
      "Epoch [20/25], Step [400/500] Loss: 0.2278\n",
      "Epoch [20/25], Step [500/500] Loss: 0.3462\n",
      "Epoch [21/25], Step [100/500] Loss: 0.3547\n",
      "Epoch [21/25], Step [200/500] Loss: 0.3489\n",
      "Epoch [21/25], Step [300/500] Loss: 0.2569\n",
      "Epoch [21/25], Step [400/500] Loss: 0.3606\n",
      "Epoch [21/25], Step [500/500] Loss: 0.2433\n",
      "Epoch [22/25], Step [100/500] Loss: 0.3516\n",
      "Epoch [22/25], Step [200/500] Loss: 0.2693\n",
      "Epoch [22/25], Step [300/500] Loss: 0.4137\n",
      "Epoch [22/25], Step [400/500] Loss: 0.2503\n",
      "Epoch [22/25], Step [500/500] Loss: 0.2936\n",
      "Epoch [23/25], Step [100/500] Loss: 0.2078\n",
      "Epoch [23/25], Step [200/500] Loss: 0.2959\n",
      "Epoch [23/25], Step [300/500] Loss: 0.3104\n",
      "Epoch [23/25], Step [400/500] Loss: 0.3976\n",
      "Epoch [23/25], Step [500/500] Loss: 0.2358\n",
      "Epoch [24/25], Step [100/500] Loss: 0.2636\n",
      "Epoch [24/25], Step [200/500] Loss: 0.2617\n",
      "Epoch [24/25], Step [300/500] Loss: 0.3187\n",
      "Epoch [24/25], Step [400/500] Loss: 0.2346\n",
      "Epoch [24/25], Step [500/500] Loss: 0.3296\n",
      "Epoch [25/25], Step [100/500] Loss: 0.2303\n",
      "Epoch [25/25], Step [200/500] Loss: 0.2468\n",
      "Epoch [25/25], Step [300/500] Loss: 0.2587\n",
      "Epoch [25/25], Step [400/500] Loss: 0.3130\n",
      "Epoch [25/25], Step [500/500] Loss: 0.2868\n"
     ]
    }
   ],
   "source": [
    "decay = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Decay the learning rate every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        decay+=1\n",
    "        optimizer.param_groups[0]['lr'] = learning_rate * (0.5**decay)\n",
    "        print(\"The new learning rate is {}\".format(optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 86.8 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
